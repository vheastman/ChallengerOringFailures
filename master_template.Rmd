---
title: "W271 Lab 1"
author: 
  - "Gurdit Chahal"
  - "Zach Day"
  - "Victoria Eastman"
date: "October 1, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, fig.width = 8, fig.height = 4)
```

## I. Introduction

Following the Challenger Space Shuttle's destruction in 1986, a commission appointed by President Reagan determined the cause to be a gas leak through a field joint. This problem was well-known to NASA and is frequently referred to as an o-ring failure. In 1989, Dalal et al. collected data from previous space shuttle launches to study the probability of an o-ring failure under conditions similar to those that occured during the Challenger launch in 1986. In this analysis, we used their dataset to echo the purpose of their study and attempt to determine the effect of key explanatory variables (temperature and pressure) on o-ring failure. In the end we specified a logistic regression model on temperature with the following formula:

The probability of an O-ring failure at temperature 31◦, the temperature when Challenger was launched, is 0.8178 with a 95% confidence interval of (0.1596, 0.9907) using profile LR intervals, which would correspond to about five out of the six primary field O-rings failing.


We first begin our analysis with a thorough exploratory data analysis in order to understand the variables we are working with. Then, we estimate a series of models that we use to predict o-ring failure. 

```{r}
# Import libraries
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(Hmisc))
suppressPackageStartupMessages(library(car))

setwd("/Users/gurditchahal/w271_lab1")
#setwd("/home/victoriaeastman/berkeley/w271/w271_lab1")
df <- read.csv("challenger.csv")
```

## II. EDA

### II (a) Data Summary and Overview

```{r}
# Start with basic looks at the data
glimpse(df)
describe(df[,c("Temp", "Pressure", "O.ring")])
```

A glimpse of the data shows we have 23 observations and 5 variables in our dataset. There are also no missing values. Again, we are primarily interested in the effects of temperature and pressure on o-ring failure. The dependent variable : the percentage of O-ring failures is *O.ring/Number* where *1) O.ring* is the number of primary field O-ring failures and *2)Number* is the total number of primary field O-rings (six total, three for each for two booster rockets). Our two potential explanatory variables are *1)Temp* is temperature at launch in degrees Fahrenheit and *2)Pressure* is combustion pressure (psi).

We can see that the `O.ring` variable has 3 distinct values: 0, 1, and 2 but only 2 observations for the value 2. To deal with this concern in representation we want to consider formulating our problem in terms of having  *at least one* o-ring failure. Therefore, for one version of our model we will binarize our outcomes and transform those flights with 2 failures as having 1 failure for the purposes of this study.We will later compare this to the binomial model when finalzing our model selection. 

```{r}
# Keep raw counts as new variable of total
df$O.ring.total = df$O.ring
# Binary version
df$O.ring[df$O.ring > 1] = 1
```

In addition, we see that the dataset contains 23 data points from other shuttle launches and none of the variables are missing any entries. Interestingly, pressure is generally considered to be a continuous variable, however, we see three distinct values of 50, 100, and 200. We could potentially see reason to use this as a categorical variable in the regression estimation below. 

```{r}
#quick check for potential collinearity as well as surface level relations
cor(df[,c("Temp", "Pressure", "O.ring")])
```

We take a quick look at correlation between variables to assess wether there might be collinearity as well as a rough gauge of predictive power between these variables prior to any transformations. We see that there is no perfect collinearity. We see a moderate negative correlation between O.ring failures and temperature and a weakly positive correlation between pressure and failures- leading us to our first consideration of these variables as the explanatory variables in our model. We see negligible positive correlation between temperature and pressure and thus don't worry about colliniearity.


### II (b) Temperature
```{r}
#What does the distribution of temperatures look like?
ggplot(df, aes(x = Temp)) +
  geom_histogram(aes(y = ..density..), binwidth = 1, fill="#0072B2", colour="black") +
  ggtitle("Histogram of Temperature at Launches") + 
  theme(plot.title = element_text(lineheight=1, face="bold"))
```

The distribution of the temperature explanatory variable has a slight negative skew. The mode of temperatures is 70 and the range is `r range(df$Temp)`. This is noteworthy because the temperature when Challenger was launched is 31◦F and there are no observations of how many O-ring failures occur at that temperature. Moreover,there are fewer observations at lower temperatures as we look to the left of 67 (everything below has no more than one count) and is worth noting in terms of precision of prediction concerns once we reach the modeling stage.


```{r, out.height="4in", out.width="0.48\\textwidth"}
#group temperature variable by O.ring failure
  print(ggplot(df, aes(factor(O.ring.total), Temp)) +
          geom_boxplot(aes(fill = factor(O.ring.total))) + 
          geom_jitter() +
          ggtitle("Temperature by Number of O-ring Failures") + ylab('Temperature(F)') 
          + xlab('Number of O-ring Failures')+
          theme(plot.title = element_text(lineheight=1, face="bold")))

```

The boxplot above of temperature grouped by o-ring failure shows the average temperature when o-rings failed was lower than when they did not. What's interesting is we can see from the chart that when the temperature is below 65, at least one O-ring fails. Therefore, it lends some credibility to a belief that O-rings are more likely to fail when temperature is lower. Another interesting observation that jumps out is that there is one observation of 2 O-ring failures at 75 which goes against the grain of the general observation between temperature and failures. This outcome could be suspect to having some influence by our other explanatory variable of *Pressure*.

```{r}
plot(O.ring.total~Temp,data=df, main="O-ring Failures vs Temperature")
```

Directly looking at the distribution of o-ring failures to temperature, we can determine our data is not completely separated(hence traditional logistic regression is still a valid possibility for model selection). 

### II (c) Pressure

```{r}
ggplot(df, aes(x = Pressure)) + geom_histogram(aes(y = ..density..),
binwidth = 5, fill = "#0072B2", colour = "black") + ggtitle("Histogram of the Pressure at Launches")+ theme(plot.title = element_text(lineheight = 1, face = "bold")) +
theme_bw() + labs(x = "Pressure", y = "Density")
 
```
```{r}
table(df$Pressure)
```


```{r}
 print(ggplot(df, aes(factor(O.ring.total), df[,i])) +
          geom_boxplot(aes(fill = factor(O.ring.total))) + 
          geom_jitter() +
          ggtitle("Pressure by Number of O-ring Failures")) + ylab('Pressure(psi)') +xlab('Number of O-ring Failures') +
          theme(plot.title = element_text(lineheight=1, face="bold"))
```




```{r}
#how much does temperature vary by each pressure stage
ggplot(df, aes(factor(Pressure), Temp)) +
          geom_boxplot(aes(fill = factor(Pressure))) + 
          geom_jitter() +
          ggtitle( "Temperature by Pressure") + 
          theme(plot.title = element_text(lineheight=1, face="bold"))
```
```{r}
plot(Pressure~Temp,data=df, main="Pressure vs Temperature")
```

Comparing temperature and pressure directly, we can see that there is a vaguely positive relationship between temperature and pressure. Under basic gas 
laws, temperature is proportional to pressure, however, this relationship doesn't appear to hold in all cases of o-ring failure. 

```{r}
#bin pressure to 200 or not
df$P200=df$Pressure
df$P200[df$P200!=200]=0
df$P200[df$P200==200]=1
df$P200=factor(df$P200)

#how much does temperature vary by each pressure stage
ggplot(df, aes(P200, Temp)) +
          geom_boxplot(aes(fill = P200)) + 
          geom_jitter() +
          ggtitle( "Temperature by Pressure") + 
          theme(plot.title = element_text(lineheight=1, face="bold"))
```
```{r}
plot(Pressure~Temp,data=df, main="Pressure vs Temperature")
```
Above we isolate the data points where pressure is 200 psi. The right boxplot (when pressure is 200 psi) has a much wider temperature range then when pressure is either 50 psi or 100 psi. This could be due to a lack of data points when the pressure is not 200 psi. 
```{r}
#slicing by pressure, any distinct relations between failure and temperature?
ggplot(df, aes(x = factor(O.ring), y = Temp, fill = factor(O.ring))) + geom_boxplot() +
facet_wrap(~ Pressure, ncol = 3)
```
Combining all three variables into the above boxplot, we can see that almost all o-ring failures occured at 200 psi with much lower temps than those that did not fail. Without an overlay of data points, we aren't able to determine how many points make up each of these ranges. 

## III. Book Questions

### Question 4

#### 4 (a) Why is independence of each observation necessary?

*This independence assumption is necessary for deriving the likelihood-based solution as we can take products of the probabilities. Potential issues is that the quality/durability of the O-ring might be dependent on the factory or even batch that it came from (clustering) and could interfere with producing a more accurate estimate when left unaccounted for. Moreover damage in one O-ring could affect the probability of damage in subsequent O-rings (might be easier for the system to collapse as a whole).*

#### 4 (b) Estimate logistic regression model

```{r}
# Initial model
mod.fit1<-glm(formula=O.ring~Pressure+Temp,data=df,family=binomial(link = logit))
summary(mod.fit1)
#mod.fit2<-glm(formula=O.ring~Pressure+Temp+Pressure:Temp,data=df,family=binomial(link = logit))
```

Our first estimated logistic regression model is

$$
\mbox{logit}(\hat{\pi}) = 13.292360 + 0.0104\mbox{Pressure} - 0.228671\mbox{Temp}
$$

#### 4 (c) Perform LRTs to judge the importance of the explanatory variables in the model.

```{r}
Anova(mod.fit1,Test='LRT')
```

The likelihood ratio test evaluates the importance of each explanatory variable and interaction variable. For the first explanatory variable, pressure, we test the hypothesis: $H_{0}: \beta_{1} = 0$ vs $H_{a}: \beta_{1} \neq 0$. The test statistic is $-2log(\Lambda) = 1.5331$ and the p-value is 0.215648 so we fail to reject the null hypothesis that pressure has no effect on o-ring failure. For the second explanatory variable, temperature, we test the hypothesis: $H_{0}: \beta_{2} = 0$ vs $H_{a}: \beta_{2} \neq 0$. The test statistic is $-2log(\Lambda) = 7.7542$ and the p-value is 0.005359 so we reject the null hypothesis and conclude that there is evidence of an interaction between temperature and o-ring failure. 

#### 4 (d) Why did they remove pressure? Why could this be a problem?

*In terms of statistical significance, pressure wasn't found to be statistically significant. Potential problems could be losing precision on temp as well as in probability, especially for edge cases.*

### Question 5

Next we estimate the model with only one regressor: Temperature.

#### 5 (a) Estimate the model

$$\mbox{logit}(\pi)=\beta_0+\beta_{1}\mbox{Temp}$$

```{r}
mod.fit2<-glm(formula=O.ring~Temp,data=df,family=binomial(link = logit))
summary(mod.fit2)
```

The estimated logistic regression model is
$$
\mbox{logit}(\hat{\pi}) = 15.0429 - 0.2322\mbox{Temp}
$$

#### 5 (b) 
Construct two plots: (1)$\pi$ vs. Temp and (2) Expected number of failures vs. Temp. Use a temperature range of 31 to 81 on the x-axis even though the minimum temperature in the data set was 53.

```{r}
# 
w<-aggregate(formula=O.ring~Temp, data=df, FUN=sum)
n<-aggregate(formula=O.ring~Temp, data=df, FUN=length)
w.n<-data.frame(Temperature=w$Temp, Failure=w$O.ring, trials=n$O.ring, proportion=round(w$O.ring/n$O.ring,4))
head(w.n)

# Plot pi vs Temp
plot(x=w$Temp, y=w$O.ring/n$O.ring, xlab="Temp(F)", ylab="Estimated probability", panel.first=grid(col="gray", lty="dotted"), xlim=c(31,81))
curve(expr=predict(object=mod.fit2, newdata=data.frame(Temp=x), type="response"), col="red", add=TRUE)

# Plot Expected number of failures vs.Temp.
plot(x=w$Temp, y=w$O.ring, xlab="Temp(F)", ylab="Estimated number of failures", panel.first=grid(col="gray", lty="dotted"), xlim=c(31,81))
#curve(expr=predict(object=mod.fit2, newdata=data.frame(Temp=x), type="response"), col="red", add=TRUE)
```

#### 5 (c) Include the 95% Wald confidence interval bands for $\pi$ on the plot. Why are the bands much wider for lower temperatures than for higher temperatures?

```{r}
plot(x=w$Temp,y=w$O.ring/n$O.ring,xlab="Temp(F)", ylab = "Estimated probability", panel.first =grid(col = "gray", lty = "dotted"))
curve(expr=predict(object=mod.fit2,newdata=data.frame(Temp = x), type = "response"), col = "red", add=TRUE,xlim=c(31,81))

ci.pi<-function(newdata,mod.fit.obj,alpha){linear.pred <- predict(object = mod.fit.obj, newdata =newdata, type = "link", se = TRUE)
CI.lin.pred.lower <- linear.pred$fit - qnorm(p =1-alpha/2)*linear.pred$se
CI.lin.pred.upper <- linear.pred$fit + qnorm(p =1-alpha/2)*linear.pred$se

CI.pi.lower <- exp(CI.lin.pred.lower) / (1 +exp(CI.lin.pred.lower))
CI.pi.upper <- exp(CI.lin.pred.upper) / (1 +exp(CI.lin.pred.upper))
list(lower = CI.pi.lower, upper = CI.pi.upper)}

curve(expr=ci.pi(newdata=data.frame(Temp=x),mod.fit.obj = mod.fit2, alpha = 0.05)$lower, col = "blue", lty="dotdash",add=TRUE,xlim=c(31,81))

curve(expr=ci.pi(newdata=data.frame(Temp=x),mod.fit.obj = mod.fit2, alpha = 0.05)$upper, col = "blue", lty="dotdash",add=TRUE,xlim=c(31,81))
```

*Bands are wider due to change in probability across temperature gradient. There is a much steeper drop in temperature below and above 65 (similar to complete separation problem). Less of a drastic change in higher temperatures due to two "middle" values between 70 and 75. We also have fewer observations for the low temperature range.*

#### 5 (d) The temperature was 31 at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval.  Discuss what assumptions need to be made in order to apply the inference procedures.

```{r}
alpha=0.05
# Set data to predict on
predict.data <- data.frame(Temp=31) 
# Linear part of model
linear.pred=predict(object = mod.fit2, newdata = predict.data,
                      type = "link", se = TRUE)
# Estimate probability
pi.hat = exp(linear.pred$fit)/(1+exp(linear.pred$fit)) 
# Confidence interval before exponentiation
CI.lin.pred = linear.pred$fit + qnorm(p = c(alpha/2, 1-alpha/2))*linear.pred$se 
# Actual interval
CI.pi = exp(CI.lin.pred)/(1+exp(CI.lin.pred)) 
```

The estimated probability of o-ring failure at 31 F is `r round(pi.hat,4)` and the corresponding confidence interval is `r round(CI.pi,4)`. 

#### 5 (e) 

```{r}
# First, fit on observed
out <- glm(formula=O.ring~Temp, family = binomial(link = logit), data=df) 
summary(out)

# Estimated probabilities from observation-fitted model
df$pred <- predict(out,data=df$Temp, type = "response") 

n <- length(df$Temp) #sample size of 23 , like original data
nboot <- 1000 #number of bootstrap samples
pi.star <- double(nboot) #array to store probability estimates

for (i in 1:nboot) { #for each bootstrap sample
    samp_df<-df[sample(nrow(df),size=n,replace=TRUE),]
    # Generate outcome for each temperature with the estimated probability
    samp_df$O.star <- rbinom(n, 1, samp_df$pred) 
    # Fit new model on these generated outcomes
    out.star <- glm(O.star ~Temp, family = binomial(link = logit),data=samp_df) 
    test=data.frame(Temp<-72.27) #test temperature
    # Predict probability of at least one O-ring failure for test temp.
    pi.star[i] <- predict(object=out.star,newdata=test, type = "response") 
    
}
pi.star.72 <- mean(pi.star) #bootstrapped estimate of probability
ci.72 <- quantile(pi.star,c(.05,.95)) #90% confidence interval from bootstrap simulation

for (i in 1:nboot) { #for each bootstrap sample
    samp_df<-df[sample(nrow(df),size=n,replace=TRUE),]
    # Generate outcome for each temperature with the estimated probability
    samp_df$O.star <- rbinom(n, 1, samp_df$pred) 
    # Fit new model on these generated outcomes
    out.star <- glm(O.star ~Temp, family = binomial(link = logit),data=samp_df) 
    test=data.frame(Temp<-31) #test temperature
    # Predict probability of at least one O-ring failure for test temp.
    pi.star[i] <- predict(object=out.star,newdata=test, type = "response") 
    
}
pi.star.31 <- mean(pi.star) #bootstrapped estimate of probability
ci.31 <- quantile(pi.star,c(.05,.95)) #90% confidence interval from bootstrap simulation
```
The bootstrapped 90% confidence interval for 31&deg;F is `r round(ci.31,4)` and for 72.27&deg;F is `r round(ci.72)`. 

#### 5 (f) Determine if a quadratic term is needed in the model for the temperature.

```{r}
mod.fit.Ha<-glm(formula=O.ring~Temp,data=df,family=binomial(link = logit))
anova(mod.fit2,mod.fit.Ha,test="Chisq")
```

*Quadratic term fails to produce significant effect in change in residual deviance and so we fail to reject that the coefficient is actually 0 for the quadratic term.*

### IV. Estimate Linear Regression Model

#### 3 (a). Interpret the main result of your final model in terms of both odds and probability of failure

#### 3 (b). With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions. Would you use the linear regression model or binary logistic regression in this case. Please explain.

```{r}
mod.lm <- lm(O.ring ~ Temp+P200, data = df)
summary(mod.lm)
```
The estimated linear regression model is

$$
\hat{\pi} = 2.658765 + 0.001962\mbox{Pressure} - 0.038136\mbox{Temp}
$$

#### The assumptions we want to test/be on the look out for:

  1. The model is linear in it's parameters.
  2. The conditional mean of the errors is 0. 
  3. There is a random sampling of observations. *don't have choice here*
  4. There is no multi-collinearity/perfect collinearity amongst explanatory variables. *passed in EDA*
  5. The errors have common constant variance (homoscedasticity).
  6. The errors are independent of one another.
  7. The errors are normally distributed. *not required for BLUE but do need for reliable inference*

We note since are observations are less than 30, we can't make asymptotic arguments for our parameter estimates. Moreover, for some of our tests, such as Shapiro, we will likely fail to reject due to lack of data and so we must test these assumptions from multiple angles (visualizations, etc.).

  *7. The errors are normally distributed.*
```{r}
# Justification for these packages: used commonly in 203 to asses LR assumptions
suppressPackageStartupMessages(library(lmtest)) 
suppressPackageStartupMessages(library(plm)) 
suppressPackageStartupMessages(library(gvlma))

hist(mod.lm$residuals)
qqnorm(mod.lm$residuals)
qqline(mod.lm$residuals)
shapiro.test(mod.lm$residuals)
coeftest(mod.lm, vcov=vcovHC(mod.lm))
```
We see both from the histogram and the qq-plot that the distribution deviates from normality at the tail ends. We particularly see a positive skew present in our distribution. With a p-value 0.009 for the Shapiro-Wilk test we can safely reject the null hypothesis that the residuals follow a normal distribution. Hence, we can't gaurantee precision on our standard errors. We can switch to robust standard errors as done in the last line.

```{r}
summary(mod.lm$fitted.values)
```

We see that the predictions are outside of the range for a probability. This is concerning as we want to assess risk of O-ring failure and probability would be well-suited for that task. Having a range that doesn't correspond gives us little sense of what's going on as the scale becomes relatively arbitrary.

```{r}
scatterplot(mod.lm$residuals,as.numeric(df$O.ring))
```

We already knew from the binary nature of the outcome that a continuous seemed innapropriate, however the line of residuals vs the O.ring failure outcome does manage to somewhat separate distinct cases (we see 4 of the observations are on the "wrong" side of the decision boundary). A linear relation seems roughly justified.

```{r}
par(mfrow=c(2,2))
plot(mod.lm)
```

We 

  *5. The errors have common constant variance (homoscedasticity).*
```{r}
ncvTest(mod.lm)
bptest(mod.lm)
```

Both the traditional Breusch-Pagan test and the Studentized Breusch-Pagan test fail to reject the null hypothesis that the variance is homoskedastic. However the lack of even band in residuals vs fitted plot and curvature in scale-location plots suggests violation of this assumption though difficult to say due to sparsity of data. As we noted, at low sample size, these tests might have also lack the power.

  *2. The conditional mean of the errors is 0.*
  
  *6. The errors are independent of one another.*
```{r}
durbinWatsonTest(mod.lm)
par(mfrow=c(1,1))
residualPlots(mod.lm)
```
By the Durbin-Watson test, we fail to reject the null that the residuals are uncorrelated with one another. From the cross-sectional nature of the launch events, we can also feel more comfortable about this result. We also are able to reject the idea that the residuals correlate with our explanatory variables as well (slight but negligble curvature), giving some plausibility to 0 conditional mean of the errors.

```{r}
gv.mod.lm <- gvlma(mod.lm)
summary(gv.mod.lm) 
```
From the gvlma we are added some assurance in that Global Stat- fail to reject relationship between X and Y are roughly linear, 4.failed to reject the Link function was appropriate , 5. failed to reject variance of the residuals seem constant. From Skewness, we see a potential need for variable transformation but we need to keep the model comparable to the logistic version.

After review of both models, we would opt for a logistic regression for several reasons. First, the output is more desirable in that we can compare odds of failures as well as compute actual probabilities of failure whereas the linear regression goes out of range. Moreover, the linear regression's questionability in terms of inference (failed normality of errors), likely keeps us restricted to the observed data in terms of predictions.

#### V. Conclusion

In conclusion, we found that 
